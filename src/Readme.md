# Описание проекта

## Что было сделано

- **Синхронный парсер**: собирает данные постранично. Следующая страница обрабатывается только после получения данных с текущей страницы.

- **Асинхронный парсер**: собирает данные, не дожидаясь окончания обработки предыдущих страниц, что ускоряет сбор.

### Требования для запуска скрипта [main.py](./main.py)

- Python версии 3.7 и выше
- Установленные библиотеки из файла [requirements.txt](./requirements.txt)

### Запуск скрипта [main.py](./main.py)

- `py main.py` или `python3 main.py` — запуск без аргументов выполняется в асинхронном режиме по умолчанию.
- `--type sync` — запуск в синхронном режиме.
- `--type async` — запуск в асинхронном режиме.

## Откуда были получены данные

Данные собираются с сайта `https://quotes.toscrape.com/` из HTML-разметки. Информация об авторах ограничена их именами, дополнительные данные не собираются.

## Как осуществлялся сбор

Так как на сайте отсутствуют `fetch` запросы и JSON-данные, сбор осуществляется из HTML-разметки. Основные этапы сбора:

1. Открывается страница по адресу `https://quotes.toscrape.com/page/n/`.
2. Выбираются все элементы с классом `quote`.
3. Для каждого элемента `quote` извлекаются вложенные элементы с классами `text`, `author`, `tags`.
4. Извлекается текст цитаты, автора и тегов. Для удаления лишних пробелов используется `strip`, кавычки `“` и `”` убираются из текста цитаты.
5. Собранные данные сохраняются в массиве словарей.
6. Если элемент с классом `next` отсутствует на странице, цикл сбора прерывается.
7. После завершения сбора функция возвращает массив с данными и сохраняет его в `JSON`-файл.

## Почему был выбран тот или иной метод/инструмент

Так как сайт не имеет защиты от роботов, API или динамически загружаемых данных через `fetch`, для парсинга был выбран легковесный и быстрый `BeautifulSoup`. Асинхронный метод был выбран из-за отсутствия ограничений на количество запросов, что позволяет ускорить сбор данных.
